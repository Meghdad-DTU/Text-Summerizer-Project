{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paladin/Downloads/Text_Summerizer/Text-Summerizer-Project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    weight_decay_rate: float\n",
    "    learning_rate: float\n",
    "    batch_size: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummerizer.constants import *\n",
    "from textSummerizer.utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "            model_ckpt= config.model_ckpt,\n",
    "            num_train_epochs= params.NUM_TRAIN_EPOCHS,\n",
    "            learning_rate= params.LEARNING_RATE,\n",
    "            weight_decay_rate= params.WEIGHT_DECAY_RATE,\n",
    "            batch_size= params.BATCH_SIZE\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paladin/Downloads/Text_Summerizer/Text-Summerizer-Project/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-11 16:05:00.147070: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 16:05:00.205713: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 16:05:00.206599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 16:05:01.447874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_t5_small = TFAutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_t5_small, return_tensors='tf')\n",
    "        \n",
    "        #loading data \n",
    "        dataset_samsum_tf = load_from_disk(self.config.data_path)\n",
    "\n",
    "        train_dataset = dataset_samsum_tf[\"train\"].to_tf_dataset(\n",
    "             batch_size= self.config.batch_size,\n",
    "             columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "             shuffle=True,\n",
    "            collate_fn=seq2seq_data_collator,\n",
    "            )\n",
    "        \n",
    "        validation_dataset = dataset_samsum_tf[\"validation\"].to_tf_dataset(\n",
    "             batch_size= self.config.batch_size,\n",
    "             columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "             shuffle=False,\n",
    "             collate_fn=seq2seq_data_collator\n",
    "             )      \n",
    "\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate= self.config.learning_rate, \n",
    "                                    weight_decay_rate= self.config.weight_decay_rate\n",
    "                                    )\n",
    "        model_t5_small.compile(optimizer= optimizer)\n",
    "\n",
    "        model_t5_small.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            epochs= self.config.num_train_epochs)\n",
    "\n",
    "        ## Save model\n",
    "        model_t5_small.save_pretrained(os.path.join(self.config.root_dir,\"t5-small-samsum-model\"))\n",
    "        ## Save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textSummerizer.exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842/1842 [==============================] - 12363s 7s/step - loss: 2.2237 - val_loss: 1.8790\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
